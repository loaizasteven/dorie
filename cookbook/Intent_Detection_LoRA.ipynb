{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Detection LoRA\n",
    "\n",
    "Sample notebook utilizing the `DORIE` package to fine tune an insurance intent detection, classification, model. This serves as an example demonstrating how to utilize the library for downstream classification tasks. This sequence classification task focus on classifying customer utterances into a multi-class intent to help the contact center route the phone call/ chat through a pre-defined or dynamic route to satify the customers needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path().resolve().parent))\n",
    "\n",
    "from libs.dorie.intent.finetune import Intent\n",
    "from libs.dorie.loader.adaptation import return_peft_model\n",
    "from libs.dorie.loader.datatokenizer import MyDataset\n",
    "\n",
    "from peft import TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "See [Synthetic data notebook](./Synthetic_Data_Generation.ipynb) for more information on generating data utilizing OpenAI API. The dataset will be loaded from HuggingFace Data Hub. You can also navigate to HuggingFace to see the [synthetic_insurance_data](https://huggingface.co/datasets/stevenloaiza/synthetic_insurance_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 785/785 [00:00<00:00, 76540.17 examples/s]\n",
      "Generating test split: 100%|██████████| 435/435 [00:00<00:00, 227865.90 examples/s]\n",
      "Map: 100%|██████████| 785/785 [00:00<00:00, 4691.39 examples/s]\n",
      "Map: 100%|██████████| 435/435 [00:00<00:00, 3302.01 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 785\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 435\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Data configuration, the intent class will load the data from the path specified here\n",
    "datapath= \"stevenloaiza/synthetic_insurance_data\"\n",
    "dataclass = MyDataset(path=datapath)\n",
    "\n",
    "# The data parameter will not be used explicitly, since it will be loader in the finetuning modules implicitly. \n",
    "# Loading it here is just to show the data that will be used for the finetuning process\n",
    "mydata = dataclass.loader()\n",
    "mydata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PEFT LoRA model\n",
    "\n",
    " * This section of the code is responsible for loading a PEFT (Parameter-Efficient Fine-Tuning) LoRA (Low-Rank Adaptation) model.\n",
    " * PEFT LoRA models are used to fine-tune large language models efficiently by adapting a small number of parameters.\n",
    " * This approach is particularly useful for tasks like intent detection, where the model needs to be adapted to understand specific intents from user inputs.\n",
    " * The model loading process typically involves specifying the model architecture, loading pre-trained weights, and preparing the model for inference or further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 888,580 || all params: 125,537,288 || trainable%: 0.7078\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"roberta-base\"\n",
    "LORA_CONFIG = {\n",
    "    \"task_type\": TaskType.SEQ_CLS, \n",
    "    \"inference_mode\": False, \n",
    "    \"r\": 8, \n",
    "    \"lora_alpha\": 32, \n",
    "    \"lora_dropout\": 0.1\n",
    "}\n",
    "lora_model = return_peft_model(\n",
    "    model_name_or_path = f\"{BASE_MODEL}\", \n",
    "    lora_config=LORA_CONFIG,\n",
    "    num_labels=dataclass.numLabels,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Intent Detection Class\n",
    "\n",
    "In this section, we will set up the intent detection class using the `Intent` class from the `DORIE` package. We will configure the class with the dataset and the PEFT LoRA model that we have loaded in the previous steps. This setup will allow us to fine-tune the model for the intent detection task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_CONFIG = {   \n",
    "    \"baseModel\": f\"{BASE_MODEL}\",\n",
    "    \"device\": \"cpu\",\n",
    "    \"modelArgs\": {\n",
    "        \"output_dir\": \"./results\",\n",
    "        \"num_train_epochs\": 10,\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"eval_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"save_total_limit\": 2,\n",
    "        \"load_best_model_at_end\": \"true\",\n",
    "        \"metric_for_best_model\": \"accuracy\",\n",
    "        \"greater_is_better\": \"true\",\n",
    "        \"save_on_each_node\": \"true\"\n",
    "    },\n",
    "    \"model\": lora_model\n",
    "}\n",
    "\n",
    "intent_classifier = Intent(\n",
    "    datapath = datapath,\n",
    "    config = TRAINING_CONFIG,\n",
    "    dataclass = dataclass,\n",
    "    # Local model trainer\n",
    "    trainer = None,\n",
    "    inference_test = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "The `train` method of the `intent_classifier` object is called to start the training process. This method will fine-tune the model using the provided dataset and configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 25/250 [00:36<04:20,  1.16s/it]\n",
      " 10%|█         | 25/250 [00:43<04:20,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2358351945877075, 'eval_accuracy': 0.46436781609195404, 'eval_runtime': 7.2618, 'eval_samples_per_second': 59.903, 'eval_steps_per_second': 1.928, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 50/250 [01:16<04:12,  1.26s/it]\n",
      " 20%|██        | 50/250 [01:24<04:12,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1695953607559204, 'eval_accuracy': 0.6229885057471264, 'eval_runtime': 7.4215, 'eval_samples_per_second': 58.613, 'eval_steps_per_second': 1.886, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 75/250 [01:57<03:21,  1.15s/it]\n",
      " 30%|███       | 75/250 [02:04<03:21,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8723050355911255, 'eval_accuracy': 0.7839080459770115, 'eval_runtime': 7.2394, 'eval_samples_per_second': 60.088, 'eval_steps_per_second': 1.934, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 100/250 [02:38<02:38,  1.06s/it]\n",
      " 40%|████      | 100/250 [02:45<02:38,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5939724445343018, 'eval_accuracy': 0.8045977011494253, 'eval_runtime': 7.0745, 'eval_samples_per_second': 61.488, 'eval_steps_per_second': 1.979, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 125/250 [03:24<02:41,  1.29s/it]\n",
      " 50%|█████     | 125/250 [03:32<02:41,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5081363916397095, 'eval_accuracy': 0.8068965517241379, 'eval_runtime': 7.3168, 'eval_samples_per_second': 59.452, 'eval_steps_per_second': 1.913, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 150/250 [04:06<02:04,  1.24s/it]\n",
      " 60%|██████    | 150/250 [04:13<02:04,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47060272097587585, 'eval_accuracy': 0.8183908045977012, 'eval_runtime': 7.5607, 'eval_samples_per_second': 57.535, 'eval_steps_per_second': 1.852, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 175/250 [04:55<01:56,  1.56s/it]\n",
      " 70%|███████   | 175/250 [05:04<01:56,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4596567451953888, 'eval_accuracy': 0.8137931034482758, 'eval_runtime': 9.2256, 'eval_samples_per_second': 47.151, 'eval_steps_per_second': 1.518, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 200/250 [05:45<01:10,  1.41s/it]\n",
      " 80%|████████  | 200/250 [05:54<01:10,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44719111919403076, 'eval_accuracy': 0.8160919540229885, 'eval_runtime': 8.6623, 'eval_samples_per_second': 50.218, 'eval_steps_per_second': 1.616, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 225/250 [06:29<00:30,  1.24s/it]\n",
      " 90%|█████████ | 225/250 [06:38<00:30,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44470515847206116, 'eval_accuracy': 0.8206896551724138, 'eval_runtime': 8.3918, 'eval_samples_per_second': 51.836, 'eval_steps_per_second': 1.668, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [07:17<00:00,  1.60s/it]\n",
      "100%|██████████| 250/250 [07:26<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44348734617233276, 'eval_accuracy': 0.8160919540229885, 'eval_runtime': 8.7468, 'eval_samples_per_second': 49.733, 'eval_steps_per_second': 1.601, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [07:27<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 447.0512, 'train_samples_per_second': 17.56, 'train_steps_per_second': 0.559, 'train_loss': 0.6921998291015625, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [07:28<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "intent_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"payPrem\",\n",
       "    \"1\": \"addDriver\",\n",
       "    \"2\": \"saleQuote\",\n",
       "    \"3\": \"startClaim\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"addDriver\": 1,\n",
       "    \"payPrem\": 0,\n",
       "    \"saleQuote\": 2,\n",
       "    \"startClaim\": 3\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"problem_type\": \"single_label_classification\",\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_classifier.config['model'].config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load To Hub\n",
    "This section demonstrates how to save the fine-tuned model to the Hugging Face Hub. This allows for easy sharing and deployment of the model. The `push_to_hub` method is used to upload the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|██████████| 3.56M/3.56M [00:00<00:00, 7.92MB/s]\n",
      "2025-01-19 20:40:12,670 - __name__ - INFO - Model saved to Hugging Face Hub as stevenloaiza/dorie-intent-classifier\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "intent_classifier.push_to_hub(model_name='stevenloaiza/dorie-intent-classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Load the learned LoRA weight tensors `A*B` that represent the lower Rank decomposition. This will be combined with the unchanged base pre-train model (recall that LoRA learns the matrices `A*B` corresponding to the update of the weight matrix `W`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"stevenloaiza/dorie-intent-classifier\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, num_labels=4)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "inference_model = PeftModel.from_pretrained(inference_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Can you add someone who's just moved in with me to our policy?\n",
      "Predicted intent: addDriver\n"
     ]
    }
   ],
   "source": [
    "inference_model.to(\"cpu\")\n",
    "_=inference_model.eval()\n",
    "\n",
    "input_text = \"Can you add someone who's just moved in with me to our policy?\"\n",
    "\n",
    "def run_inference(input_text, inference_model):\n",
    "  inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "  inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}\n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = inference_model(**inputs)\n",
    "      predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "  labelMap = {\n",
    "      0: \"payPrem\",\n",
    "      1: \"addDriver\",\n",
    "      2: \"saleQuote\",\n",
    "      3: \"startClaim\"\n",
    "    }\n",
    "  predicted_intent = labelMap[predictions.item()]\n",
    "\n",
    "  print(f\"Input text: {input_text}\")\n",
    "  print(f\"Predicted intent: {predicted_intent}\")\n",
    "\n",
    "run_inference(input_text, inference_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging LoRA model with the base model\n",
    "From the paper LoRA the forward pass yiled `W0x + BAx` which can be combined as `(W0 + BA)x => (W_LoRA * x)`, this ensure no additional overhead in latency during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Can you add someone who's just moved in with me to our policy?\n",
      "Predicted intent: addDriver\n"
     ]
    }
   ],
   "source": [
    "merged_model = inference_model.merge_and_unload()\n",
    "run_inference(input_text, merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dorie_env_ipynb",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
